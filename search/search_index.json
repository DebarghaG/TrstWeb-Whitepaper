{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TrstWeb Whitepaper v0.2 Debargha Ganguly d@dtlabsindia.com TrstWeb's mission is to enable a simple, fair, and globally scalable infrastructure for applications, data and compute. Abstract A decentralised ecosystem would be capable of providing unrestricted access to applications, secure data storage and compute capability, without relying heavily on a few 'gate-keepers' of the internet. This would re-democratize the world wide web, putting it's participant's interests first. In this paper, working up from the transport layer, we discuss the usage of Federated Byzatine Agreement - amongst nodes to reach consensus, providing secure data, resolving and serving 'trustable' applications, and middleware to support high performance parallel computation. The Internet Landscape The internet started about 4 decades ago, and back then it was almost just another research project. Over the years, it has grown to be something that's woven into the very fabric of human society and our interactions. The applications and infrastructures have changed a lot since the early days, however the basic protocols about how the internet works still bear an uncanny resemblance to how we started. [1][2][3] Sometime during the 90s, the client server model of building and running applications on the internet was popularised. This pushed the web, and web-based technologies to take off, while being dependent on central figures in the computing industry, until today, when these central figures store private user data, authenticate our credentials and provide essential services. [4] While this model has produced astronomical returns on investment, and venture capital, the waves of start-ups and the rise of the big data companies have made increasing efforts to monetize almost every human activity. The profit motive have become the primary drivers of innovation. [5] On a more abstract level, the design of products by the said central figures in the computing industry are driven by monetary gain to self and investors, there-by pushing users for more consumption, even at the risk of widespread societal impact, such as a generation on the brink of depression. [6] The reliance of the world, at scale, on such centralised technology, results in the exposure to risks that range from the loss of individual user privacy to massive data breaches and mass-control of the populous, to the point of influencing political decisions. [7] On the other hand, designing, creating and actually using a decentralised internet is often much more challenging than the previously discussed, traditional client-server model. Nodes in the network must be capable of sending messages for themselves, but they must also be able to route messages across the network, which is more complex, since each node must be able to calculate a route to the destination. Considering that each node will need more computational power to handle the additional processing steps needed to participate, security concerns that need extensive testing and scrutiny, and finally the performance and financial issues caused, since nodes would need to spend more and more time routing data rather than performing useful computation, consumer and enterprise technology must be done better. Building such software, individually, outside an ecosystem, is not feasible at scale - as dictated by our experience. With more powerful client devices, better networks connecting the world, and the rise of edge computation, the shifting of the tectonic plates towards decentralised computing has begun. Over the next few decades, by giving back the power of their own data to the people, the power of monetisation of it will shift to users from large scale tech companies which act as data silos. [9][10] We think cloud compute and storage providers will be functionally changed to just being providers of computers as a sharing economy, leading to users using their own 'always-on' computers or owning their own personal devices on cloud that are always connected and act like the user's digital counterpart. Decentralisation fundamentally changes the status quo in the software industry, tilting it to the favour of customers, optimizing their benefit over all else. With the opportunity to create a new ecosystem, a new set of protocols, abstractions and tools, this is an opportunity for improvement. [1] Tobin, James (12 June 2012). Great Projects: The Epic Story of the Building of America, from the Taming of the Mississippi to the Invention of the Internet. Simon and Schuster. ISBN 978-0-7432-1476-6. [2] Quittner, Joshua (29 March 1999). \"Network Designer Tim Berners-Lee\". Time Magazine. Archived from the original on 15 August 2007. Retrieved 17 May 2010. \"He wove the World Wide Web and created a mass medium for the 21st century. The World Wide Web is Berners-Lee's alone. He designed it. He set it loose it on the world. And he more than anyone else has fought to keep it an open, non-proprietary and free.\" [3] \"ARPANET Maps 1969 to 1977\". California State University, Dominguez Hills (CSUDH). 4 January 1978. Archived from the original on 19 April 2012. Retrieved 17 May 2012. [4] Introduction. (n.d.). Retrieved from https://oauth.net/about/introduction/ [5] Harris, Michael (2 January 2015). \"Book review: 'The Internet Is Not the Answer' by Andrew Keen\". Washington Post. Archived from the original on 20 January 2015. Retrieved 25 January 2015. [6] Lin, Liu Yi, et al. \"Association between social media use and depression among US young adults.\" Depression and anxiety 33.4 (2016): 323-331. [7] Kopfstein, Janus. \"The Mission To Decentralize The Internet\". The New Yorker. Cond\u00e9 Nast. Archived from the original on 13 December 2013. Retrieved 13 December 2013 [8] Barkai, David (2001). Peer-to-peer computing : technologies for sharing and collaborating on the net. Hillsboro, OR: Intel Press. ISBN 978-0970284679. OCLC 49354877. [9] \"Globally Distributed Content Delivery, by J. Dilley, B. Maggs, J. Parikh, H. Prokop, R. Sitaraman and B. Weihl, IEEE Internet Computing, Volume 6, Issue 5, November 2002\" (PDF). Archived (PDF) from the original on 2017-08-09. Retrieved 2019-10-25. [10] Anderson, Janna, and Lee Rainie. \"The future of well-being in a tech-saturated world.\" Pew Research Center (2018).","title":"Home"},{"location":"#the-internet-landscape","text":"The internet started about 4 decades ago, and back then it was almost just another research project. Over the years, it has grown to be something that's woven into the very fabric of human society and our interactions. The applications and infrastructures have changed a lot since the early days, however the basic protocols about how the internet works still bear an uncanny resemblance to how we started. [1][2][3] Sometime during the 90s, the client server model of building and running applications on the internet was popularised. This pushed the web, and web-based technologies to take off, while being dependent on central figures in the computing industry, until today, when these central figures store private user data, authenticate our credentials and provide essential services. [4] While this model has produced astronomical returns on investment, and venture capital, the waves of start-ups and the rise of the big data companies have made increasing efforts to monetize almost every human activity. The profit motive have become the primary drivers of innovation. [5] On a more abstract level, the design of products by the said central figures in the computing industry are driven by monetary gain to self and investors, there-by pushing users for more consumption, even at the risk of widespread societal impact, such as a generation on the brink of depression. [6] The reliance of the world, at scale, on such centralised technology, results in the exposure to risks that range from the loss of individual user privacy to massive data breaches and mass-control of the populous, to the point of influencing political decisions. [7] On the other hand, designing, creating and actually using a decentralised internet is often much more challenging than the previously discussed, traditional client-server model. Nodes in the network must be capable of sending messages for themselves, but they must also be able to route messages across the network, which is more complex, since each node must be able to calculate a route to the destination. Considering that each node will need more computational power to handle the additional processing steps needed to participate, security concerns that need extensive testing and scrutiny, and finally the performance and financial issues caused, since nodes would need to spend more and more time routing data rather than performing useful computation, consumer and enterprise technology must be done better. Building such software, individually, outside an ecosystem, is not feasible at scale - as dictated by our experience. With more powerful client devices, better networks connecting the world, and the rise of edge computation, the shifting of the tectonic plates towards decentralised computing has begun. Over the next few decades, by giving back the power of their own data to the people, the power of monetisation of it will shift to users from large scale tech companies which act as data silos. [9][10] We think cloud compute and storage providers will be functionally changed to just being providers of computers as a sharing economy, leading to users using their own 'always-on' computers or owning their own personal devices on cloud that are always connected and act like the user's digital counterpart. Decentralisation fundamentally changes the status quo in the software industry, tilting it to the favour of customers, optimizing their benefit over all else. With the opportunity to create a new ecosystem, a new set of protocols, abstractions and tools, this is an opportunity for improvement. [1] Tobin, James (12 June 2012). Great Projects: The Epic Story of the Building of America, from the Taming of the Mississippi to the Invention of the Internet. Simon and Schuster. ISBN 978-0-7432-1476-6. [2] Quittner, Joshua (29 March 1999). \"Network Designer Tim Berners-Lee\". Time Magazine. Archived from the original on 15 August 2007. Retrieved 17 May 2010. \"He wove the World Wide Web and created a mass medium for the 21st century. The World Wide Web is Berners-Lee's alone. He designed it. He set it loose it on the world. And he more than anyone else has fought to keep it an open, non-proprietary and free.\" [3] \"ARPANET Maps 1969 to 1977\". California State University, Dominguez Hills (CSUDH). 4 January 1978. Archived from the original on 19 April 2012. Retrieved 17 May 2012. [4] Introduction. (n.d.). Retrieved from https://oauth.net/about/introduction/ [5] Harris, Michael (2 January 2015). \"Book review: 'The Internet Is Not the Answer' by Andrew Keen\". Washington Post. Archived from the original on 20 January 2015. Retrieved 25 January 2015. [6] Lin, Liu Yi, et al. \"Association between social media use and depression among US young adults.\" Depression and anxiety 33.4 (2016): 323-331. [7] Kopfstein, Janus. \"The Mission To Decentralize The Internet\". The New Yorker. Cond\u00e9 Nast. Archived from the original on 13 December 2013. Retrieved 13 December 2013 [8] Barkai, David (2001). Peer-to-peer computing : technologies for sharing and collaborating on the net. Hillsboro, OR: Intel Press. ISBN 978-0970284679. OCLC 49354877. [9] \"Globally Distributed Content Delivery, by J. Dilley, B. Maggs, J. Parikh, H. Prokop, R. Sitaraman and B. Weihl, IEEE Internet Computing, Volume 6, Issue 5, November 2002\" (PDF). Archived (PDF) from the original on 2017-08-09. Retrieved 2019-10-25. [10] Anderson, Janna, and Lee Rainie. \"The future of well-being in a tech-saturated world.\" Pew Research Center (2018).","title":"The Internet Landscape"},{"location":"1_Establishing_Consensus/1_TrstWeb-as-a-distributed-system/","text":"The TrstWeb as a Distributed System The TrstWeb ecosystem, is a distributed system - that is a collection of independent entities that cooperate to establish consensus, and solve problems based on the primitives built into the system. Distributed systems have been in existence since the start of the universe - and would include everything from systems of microorganisms, to flocks of birds. There are usually characterized by communication amongst intelligent mobile systems in nature. One of the marvels of modern-computing is the inter-domain routing - a set of protocols that have come together to scale-up the number of nodes on the network, exponentially with time. In this whitepaper, we derive our design decisions from systems such as these, Node Architecture Each node on our network is a memory-processing unit, and is connected using a communication network. The distributed software ( in the form of the middleware libraries) that are running on each node- are the TrstWeb Software, and is undergoing distributed execution. This software is the middleware that drives the distributed system, and provides transparency of homogenity at the platform level. Defining Targets Inherently distributed computing : Reaching multiparty consensus, amongst geographically distant parties to transfer of safe data - the computation is inherently distributed. Resource sharing : Setting up massive compute infrastructure, or replication of extensive amounts of data across the nodes is neither cost-effective or practical. Besides, these may create unexpected bottle-necking. Increased Reliability : This can be of multiple types : Liveness : Availability of the resource at all times. Safety : Correctness of the state of the resource. Fault Tolerance : Ability to recover from system failures. Increased scalability and performance/cost ratio : Current decentralised networks don't provide internet level scalability - to allow widespread adoption. Expensive consensus mechanisms also mean that the performance to cost ratio can be improved.","title":"The TrstWeb as a Distributed System"},{"location":"1_Establishing_Consensus/1_TrstWeb-as-a-distributed-system/#the-trstweb-as-a-distributed-system","text":"The TrstWeb ecosystem, is a distributed system - that is a collection of independent entities that cooperate to establish consensus, and solve problems based on the primitives built into the system. Distributed systems have been in existence since the start of the universe - and would include everything from systems of microorganisms, to flocks of birds. There are usually characterized by communication amongst intelligent mobile systems in nature. One of the marvels of modern-computing is the inter-domain routing - a set of protocols that have come together to scale-up the number of nodes on the network, exponentially with time. In this whitepaper, we derive our design decisions from systems such as these,","title":"The TrstWeb as a Distributed System"},{"location":"1_Establishing_Consensus/1_TrstWeb-as-a-distributed-system/#node-architecture","text":"Each node on our network is a memory-processing unit, and is connected using a communication network. The distributed software ( in the form of the middleware libraries) that are running on each node- are the TrstWeb Software, and is undergoing distributed execution. This software is the middleware that drives the distributed system, and provides transparency of homogenity at the platform level.","title":"Node Architecture"},{"location":"1_Establishing_Consensus/1_TrstWeb-as-a-distributed-system/#defining-targets","text":"Inherently distributed computing : Reaching multiparty consensus, amongst geographically distant parties to transfer of safe data - the computation is inherently distributed. Resource sharing : Setting up massive compute infrastructure, or replication of extensive amounts of data across the nodes is neither cost-effective or practical. Besides, these may create unexpected bottle-necking. Increased Reliability : This can be of multiple types : Liveness : Availability of the resource at all times. Safety : Correctness of the state of the resource. Fault Tolerance : Ability to recover from system failures. Increased scalability and performance/cost ratio : Current decentralised networks don't provide internet level scalability - to allow widespread adoption. Expensive consensus mechanisms also mean that the performance to cost ratio can be improved.","title":"Defining Targets"},{"location":"1_Establishing_Consensus/2_Consensus_Protocol/","text":"Understading Consensus An Introduction to Consensus Agreement amongst the different nodes in our system is a fundamental requirement. In nature, this occurs by the exchange of information and negotiation amongst those involved to reach a common understanding or agreement, before taking an irreversible action. Byzatine Fault Tolerance Amongst the n processes of the system, at most f of them can be faulty. A faulty process can behave in any manner allowed by the failure model. The Byzatine failure model, has been chosen, because fail-stop models and send/receive omission models have a probability of behaving unpredictably. As the scientific literature shows, consensus around an asynchronous system can't be established, hence we're narrowing down to synchronous systems in which the non-arrival of an expected message can be dealt with by assuming the arrival of a message with default data, and moving on to the next round of the algorithm. Here we are considering full logical connectivity within the local cluster, where each node can communicate with another with the help of message passing. We'll therefore be using our protocol to solve the Byzatine Agreement Problem , the consensus problem and the interactive consistency problem , interchangeably, as a generalisation. Authentication, Scheduling and Assumptions Authentication, discussed further in the paper, ensures that every node knows the identity of the sender process. The protocol, also uses an authentication in the form of a digital signature, which makes sure that if a node forges a message or tampers with the contents of the received message before it has been relayed, the recipient can detect the forgery or tampering - thereby reducing the damage that faulty nodes can cause. We're also assuming that the different rounds of the consensus protocol, are dealt with in the form of sub-rounds with the message passing being synchronised with the help of a scheduler. A simplification of the problem statement, can be yielded by assuming that the channel is reliable, therefore meaning channel instabilities are handled by our byzatine failure model. The TrstWeb Consensus Protocol The TRSTWeb Consensus Protocol relies on an extended form of the Federated Byzantine Agreement, currently in production, with Ripple and the Stellar Consensus Protocol. The extension to the aforementioned FBA protocol, lie in the domain of hierarchial clustering of nodes, and leader election, therefore preserving the original theoretical proofs of security. Let's consider that our global system, consists of a loose confederation of nodes, each of which have chosen their own set of nodes they trust globally - each running the TrstWeb software as the middleware. Using a replicated state machine approach towards managing secure data, we must establish consensus first. The problem statement, defined, would be to uniformly manage said replication, across all the nodes uniformly, such that it remains in sync. Now, all the replicas rely on the same state of the system, and agree on the same exact sequence of deterministic operations, so as to arrive at the next state. Simplifying further, heuristically, the question comes down to if the node V can apply the update x on the slot n . System wide agreement on a system is possible when a sufficient threshold of messages have been passed through the network. We also assume that irreversible actions happen every time an action is committed by a node, meaning the node can't change it's history. This forms the backbone of the trust mechanism, and is also reffered to as the blockchain, or the distributed leger. Scoping System Requirements The consensus problem based on top of the byzatine agreement problem requires a node with an initial state to reach an agreement in the form of a single value, with all the other nodes in the system. Formally, in the system, we'd like it to have safety in terms of both agreement and validity, as well as termination. Termination : All non-faulty processes eventually decide on a value Agreement : All processes that decide do so on the same value Validity : The value that has been decided must have proposed by some process Due to the FLP impossibility result, however, we can't achieve all of the above. We'll be exploring possible tradeoffs further in the paper.","title":"Understading Consensus"},{"location":"1_Establishing_Consensus/2_Consensus_Protocol/#understading-consensus","text":"","title":"Understading Consensus"},{"location":"1_Establishing_Consensus/2_Consensus_Protocol/#an-introduction-to-consensus","text":"Agreement amongst the different nodes in our system is a fundamental requirement. In nature, this occurs by the exchange of information and negotiation amongst those involved to reach a common understanding or agreement, before taking an irreversible action.","title":"An Introduction to Consensus"},{"location":"1_Establishing_Consensus/2_Consensus_Protocol/#byzatine-fault-tolerance","text":"Amongst the n processes of the system, at most f of them can be faulty. A faulty process can behave in any manner allowed by the failure model. The Byzatine failure model, has been chosen, because fail-stop models and send/receive omission models have a probability of behaving unpredictably. As the scientific literature shows, consensus around an asynchronous system can't be established, hence we're narrowing down to synchronous systems in which the non-arrival of an expected message can be dealt with by assuming the arrival of a message with default data, and moving on to the next round of the algorithm. Here we are considering full logical connectivity within the local cluster, where each node can communicate with another with the help of message passing. We'll therefore be using our protocol to solve the Byzatine Agreement Problem , the consensus problem and the interactive consistency problem , interchangeably, as a generalisation.","title":"Byzatine Fault Tolerance"},{"location":"1_Establishing_Consensus/2_Consensus_Protocol/#authentication-scheduling-and-assumptions","text":"Authentication, discussed further in the paper, ensures that every node knows the identity of the sender process. The protocol, also uses an authentication in the form of a digital signature, which makes sure that if a node forges a message or tampers with the contents of the received message before it has been relayed, the recipient can detect the forgery or tampering - thereby reducing the damage that faulty nodes can cause. We're also assuming that the different rounds of the consensus protocol, are dealt with in the form of sub-rounds with the message passing being synchronised with the help of a scheduler. A simplification of the problem statement, can be yielded by assuming that the channel is reliable, therefore meaning channel instabilities are handled by our byzatine failure model.","title":"Authentication, Scheduling and Assumptions"},{"location":"1_Establishing_Consensus/2_Consensus_Protocol/#the-trstweb-consensus-protocol","text":"The TRSTWeb Consensus Protocol relies on an extended form of the Federated Byzantine Agreement, currently in production, with Ripple and the Stellar Consensus Protocol. The extension to the aforementioned FBA protocol, lie in the domain of hierarchial clustering of nodes, and leader election, therefore preserving the original theoretical proofs of security. Let's consider that our global system, consists of a loose confederation of nodes, each of which have chosen their own set of nodes they trust globally - each running the TrstWeb software as the middleware. Using a replicated state machine approach towards managing secure data, we must establish consensus first. The problem statement, defined, would be to uniformly manage said replication, across all the nodes uniformly, such that it remains in sync. Now, all the replicas rely on the same state of the system, and agree on the same exact sequence of deterministic operations, so as to arrive at the next state. Simplifying further, heuristically, the question comes down to if the node V can apply the update x on the slot n . System wide agreement on a system is possible when a sufficient threshold of messages have been passed through the network. We also assume that irreversible actions happen every time an action is committed by a node, meaning the node can't change it's history. This forms the backbone of the trust mechanism, and is also reffered to as the blockchain, or the distributed leger.","title":"The TrstWeb Consensus Protocol"},{"location":"1_Establishing_Consensus/2_Consensus_Protocol/#scoping-system-requirements","text":"The consensus problem based on top of the byzatine agreement problem requires a node with an initial state to reach an agreement in the form of a single value, with all the other nodes in the system. Formally, in the system, we'd like it to have safety in terms of both agreement and validity, as well as termination. Termination : All non-faulty processes eventually decide on a value Agreement : All processes that decide do so on the same value Validity : The value that has been decided must have proposed by some process Due to the FLP impossibility result, however, we can't achieve all of the above. We'll be exploring possible tradeoffs further in the paper.","title":"Scoping System Requirements"},{"location":"1_Establishing_Consensus/3_Flexible Trust/","text":"Flexible Trust Quorums and Quorum Slices For the purpose of allowing flexible models of trust, we need to introduce two terminologies, we need to understand Quorum and Quorum Slices. To understand the concept best, let's understand it from the perspective of human society. For a certain fact to be accepted universally, amongst all, as truth, Quorum is the number of minimum people who need to agree to it. Similarly, in our network, for an update to be implemented across the entire network, the quorum would be the minimum number of nodes that need to accept the update to the state machine. On the other hand, quorum slices are a more individual abstraction of trust. Let's assume , like in society, a person X, only trusts the people closest to them, individuals who have proven themselves to be friends, and each of these friends are free to make their own decisions about who their friends are. Information would then be propagated by intersections of these friend circles, the people who are trusted by both friend circles. Similarly the set of nodes in our network, that node X trusts, is defined as the quorum slice it's in. System wide consensus will be established when there is good quorum intersection. This method was first proposed by Mazieres in the Stellar Consensus Protocol, as a generalisation of the Ripple consensus protocol, which uses subnetworks. However, quorum based mutual exclusion algorithms have been around for a while now - which also reduce the message complexity by having nodes ask for permission only from a subset of total nodes in the network. Disjoint Quorums Intersecting Quorums Building on top of the definitions of coteries and quorums, the formal definition can be given as the following. A coterie C is defined as a set of sets, where each set g \u2208 C is called a quorum, where the following properties hold for the quorums in a coterie. Quorum slices are slices of the Quorum. Intersection Property : For every quorum g, h \u2208 C, g \u22c2 h \u2260 \u00d8. Minimality Property : There should be no quorums g,h in coterie C such that g \u2287 h. For consensus, we note that the intersection property, due to the common nodes, the safety of the system is guaranteed. At the same time, the minimality property ensures efficiency rather than correctness, and quorums are formed as sets that contain a majority of nodes.","title":"Flexible Trust"},{"location":"1_Establishing_Consensus/3_Flexible Trust/#flexible-trust","text":"","title":"Flexible Trust"},{"location":"1_Establishing_Consensus/3_Flexible Trust/#quorums-and-quorum-slices","text":"For the purpose of allowing flexible models of trust, we need to introduce two terminologies, we need to understand Quorum and Quorum Slices. To understand the concept best, let's understand it from the perspective of human society. For a certain fact to be accepted universally, amongst all, as truth, Quorum is the number of minimum people who need to agree to it. Similarly, in our network, for an update to be implemented across the entire network, the quorum would be the minimum number of nodes that need to accept the update to the state machine. On the other hand, quorum slices are a more individual abstraction of trust. Let's assume , like in society, a person X, only trusts the people closest to them, individuals who have proven themselves to be friends, and each of these friends are free to make their own decisions about who their friends are. Information would then be propagated by intersections of these friend circles, the people who are trusted by both friend circles. Similarly the set of nodes in our network, that node X trusts, is defined as the quorum slice it's in. System wide consensus will be established when there is good quorum intersection. This method was first proposed by Mazieres in the Stellar Consensus Protocol, as a generalisation of the Ripple consensus protocol, which uses subnetworks. However, quorum based mutual exclusion algorithms have been around for a while now - which also reduce the message complexity by having nodes ask for permission only from a subset of total nodes in the network. Disjoint Quorums Intersecting Quorums Building on top of the definitions of coteries and quorums, the formal definition can be given as the following. A coterie C is defined as a set of sets, where each set g \u2208 C is called a quorum, where the following properties hold for the quorums in a coterie. Quorum slices are slices of the Quorum. Intersection Property : For every quorum g, h \u2208 C, g \u22c2 h \u2260 \u00d8. Minimality Property : There should be no quorums g,h in coterie C such that g \u2287 h. For consensus, we note that the intersection property, due to the common nodes, the safety of the system is guaranteed. At the same time, the minimality property ensures efficiency rather than correctness, and quorums are formed as sets that contain a majority of nodes.","title":"Quorums and Quorum Slices"},{"location":"1_Establishing_Consensus/4_Voting/","text":"Voting Voting and Confirmation Rounds We'll be discussing how the nodes come together to vote, and agree on a given statement. As discussed by Mazieres, and Skeen and Stonebraker 1983, from the node's perspective, it's divided into three phases: unknown, accepted and confirmed. At the very beginning, the status for the slot is not known to the node - which means it could be acceptable, rejectable or even result in a stuck state. If the first voting across the nodes comes through to an acceptance, the node is said to have accepted the message. Just because the node has accepted the message, however doesn't mean that all the other nodes have accepted it - or if it has been manipulated, then accepting the statement doesn't mean that the rest of the nodes which aren't damaged may think the statement is false. A second vote asks for confirmation whether - the statement that is to be accepted, is actually accepted by a majority of people in it's quorum slice. Therefore, this is when the node moves on towards actually moving the statement as confirmed. Liveness Guarantees Considering a terminating execution for a system which can have n=2 possible states, where m is the last message that was passed, and where n=1 * states is the state of consensus *, we can say that the message m was the deciding message. A challenge in establishing consensus, is that any of the statements - the voting is occurring on can go into a stuck state, before a consensus is established. Therefore, if we need to make sure that the statement m, that is the deciding statement doesn't decide the fate of the system anymore, as voting on the value - is not a good idea. We therefore must be able to neutralise a statement - i.e. making sure that a stuck statement doesn't interrupt the liveness of the statement. By making sure that all the voting is happening on statements that never get stuck, and statements who's hold on the consensus question can be broken, if stuck, we're making a tradeoff in terms of guaranteeing liveness. We don't need provable termination, but termination in practice is good enough, by following the procedure mentioned above. At the end of the day, our network must absolutely avoid stuck states. Statements that never get stuck, therefore, must be voted on. In order to create such neutralisable statements, we'll be using the ballot-based approach, invented by Paxos [Lamport 1998]. Ballot-based protocols associate the values in votes with monotonically increasing ballot numbers. Should a ballot get stuck, nodes retry the same slot with a higher ballot, taking care never to select values that would contradict prior stuck ballots. Because the nodes can't change their own values and if nobody votes against the statement, the network can't tend towards a stuck state, or statements who's hold on consensus questions can be broken. Comments : Therefore, with a generalised implementation of this protocol, we can deal with both public-ly open systems, as well as permissioned networks, under a generalised approach, therefore providing decentralised control, low latencies for adding to the block, flexible models of trust with a variety of available choices for every node, as well as asymptotic security. This forms a part of the core of the TrstWeb, with applications from Data Storage, to consensus about the applications being served, distributed computing capabilites, and off-chain encrypted data storage.","title":"Voting"},{"location":"1_Establishing_Consensus/4_Voting/#voting","text":"","title":"Voting"},{"location":"1_Establishing_Consensus/4_Voting/#voting-and-confirmation-rounds","text":"We'll be discussing how the nodes come together to vote, and agree on a given statement. As discussed by Mazieres, and Skeen and Stonebraker 1983, from the node's perspective, it's divided into three phases: unknown, accepted and confirmed. At the very beginning, the status for the slot is not known to the node - which means it could be acceptable, rejectable or even result in a stuck state. If the first voting across the nodes comes through to an acceptance, the node is said to have accepted the message. Just because the node has accepted the message, however doesn't mean that all the other nodes have accepted it - or if it has been manipulated, then accepting the statement doesn't mean that the rest of the nodes which aren't damaged may think the statement is false. A second vote asks for confirmation whether - the statement that is to be accepted, is actually accepted by a majority of people in it's quorum slice. Therefore, this is when the node moves on towards actually moving the statement as confirmed.","title":"Voting and Confirmation Rounds"},{"location":"1_Establishing_Consensus/4_Voting/#liveness-guarantees","text":"Considering a terminating execution for a system which can have n=2 possible states, where m is the last message that was passed, and where n=1 * states is the state of consensus *, we can say that the message m was the deciding message. A challenge in establishing consensus, is that any of the statements - the voting is occurring on can go into a stuck state, before a consensus is established. Therefore, if we need to make sure that the statement m, that is the deciding statement doesn't decide the fate of the system anymore, as voting on the value - is not a good idea. We therefore must be able to neutralise a statement - i.e. making sure that a stuck statement doesn't interrupt the liveness of the statement. By making sure that all the voting is happening on statements that never get stuck, and statements who's hold on the consensus question can be broken, if stuck, we're making a tradeoff in terms of guaranteeing liveness. We don't need provable termination, but termination in practice is good enough, by following the procedure mentioned above. At the end of the day, our network must absolutely avoid stuck states. Statements that never get stuck, therefore, must be voted on. In order to create such neutralisable statements, we'll be using the ballot-based approach, invented by Paxos [Lamport 1998]. Ballot-based protocols associate the values in votes with monotonically increasing ballot numbers. Should a ballot get stuck, nodes retry the same slot with a higher ballot, taking care never to select values that would contradict prior stuck ballots. Because the nodes can't change their own values and if nobody votes against the statement, the network can't tend towards a stuck state, or statements who's hold on consensus questions can be broken.","title":"Liveness Guarantees"},{"location":"1_Establishing_Consensus/4_Voting/#comments","text":"Therefore, with a generalised implementation of this protocol, we can deal with both public-ly open systems, as well as permissioned networks, under a generalised approach, therefore providing decentralised control, low latencies for adding to the block, flexible models of trust with a variety of available choices for every node, as well as asymptotic security. This forms a part of the core of the TrstWeb, with applications from Data Storage, to consensus about the applications being served, distributed computing capabilites, and off-chain encrypted data storage.","title":"Comments :"},{"location":"1_Establishing_Consensus/5_SPEC/","text":"TrstConsensus Spec Fault Tolerance Since our protocol ensures Byzatine Agreement, it solves the consensus problem for a system of N nodes, where we can say N = 3f + 1, for any positive integer f, therefore guaranteeing safety and liveness, as long as at the maxiumum, the f nodes are faulty. Open Membership FBA is special because it works with an open membership, meaning different nodes can have completely different ideas about which nodes they trust, and what their quorum needs. This closely represents human systems of trust, and was therefore chosen. A quorum has to be a non-empty set of nodes that contain atleast one quorum slice for each of the non-faulty nodes. Safety and Liveness Guarantees If all of the nodes choose the same quorum slice, then FBA is just Byzatine Agreement, as a mechanism that ensures agreement amongst all the non-faulty nodes in the system. When the nodes each choose their own quorum slices, safety may only be guaranteed for a subset of them. Safety is dependent on quorum overlap, but is now an individual property instead of a system-wide property. A FBA system can only guarantee agreement between two different nodes in the system, in case every quorum containing node 1 and every quorum containing node 2, intersect for atleast one non-faulty node. FBA also guarantees liveness under eventual synchrony for sets of nodes, where where safety is guaranteed. Combining with Leader Election On top of the federated voting, that has been described in the previous set of sections too, there exists a leader election system, such that the system can narrow down on a few or one suggestion to the replicated state machine. Without this, every node may suggest a different value for the slot, and because of our liveness guarantees, the nodes will keep moving to the next round, however making very little practical progress by agreeing to decisions. On the other hand, Federated Voting confirms a nominated change, by the leaders chosen by the leader election, with reasonable confidence that nodes for which security is guaranteed will never confirm contradictory statements. Stages of obtaining Consensus Pre-commit phase : Predliminary declaration of votes. - Can't vote on a ballot without preparing the transaction (n,x) and dropping (n', x') Individual Acceptance - Accept and broadcast to the other nodes that the individual acceptance has occured. Broadcast to all it's nodes about details ( in quorum slice ) Ratification : Nodes vote to accept a common update. - If more than quorum inside the quorum slice has been achieved, accept. If not correct, check if quorum achieved on any of the other data broadcast to all it's nodes about details of accepted transaction Finally, verify that transaction has been accepted in the other nodes. Broadcast : - Make the updated ledger and broadcast amongst all the nodes.","title":"TrstConsensus Spec"},{"location":"1_Establishing_Consensus/5_SPEC/#trstconsensus-spec","text":"","title":"TrstConsensus Spec"},{"location":"1_Establishing_Consensus/5_SPEC/#fault-tolerance","text":"Since our protocol ensures Byzatine Agreement, it solves the consensus problem for a system of N nodes, where we can say N = 3f + 1, for any positive integer f, therefore guaranteeing safety and liveness, as long as at the maxiumum, the f nodes are faulty.","title":"Fault Tolerance"},{"location":"1_Establishing_Consensus/5_SPEC/#open-membership","text":"FBA is special because it works with an open membership, meaning different nodes can have completely different ideas about which nodes they trust, and what their quorum needs. This closely represents human systems of trust, and was therefore chosen. A quorum has to be a non-empty set of nodes that contain atleast one quorum slice for each of the non-faulty nodes.","title":"Open Membership"},{"location":"1_Establishing_Consensus/5_SPEC/#safety-and-liveness-guarantees","text":"If all of the nodes choose the same quorum slice, then FBA is just Byzatine Agreement, as a mechanism that ensures agreement amongst all the non-faulty nodes in the system. When the nodes each choose their own quorum slices, safety may only be guaranteed for a subset of them. Safety is dependent on quorum overlap, but is now an individual property instead of a system-wide property. A FBA system can only guarantee agreement between two different nodes in the system, in case every quorum containing node 1 and every quorum containing node 2, intersect for atleast one non-faulty node. FBA also guarantees liveness under eventual synchrony for sets of nodes, where where safety is guaranteed.","title":"Safety and Liveness Guarantees"},{"location":"1_Establishing_Consensus/5_SPEC/#combining-with-leader-election","text":"On top of the federated voting, that has been described in the previous set of sections too, there exists a leader election system, such that the system can narrow down on a few or one suggestion to the replicated state machine. Without this, every node may suggest a different value for the slot, and because of our liveness guarantees, the nodes will keep moving to the next round, however making very little practical progress by agreeing to decisions. On the other hand, Federated Voting confirms a nominated change, by the leaders chosen by the leader election, with reasonable confidence that nodes for which security is guaranteed will never confirm contradictory statements.","title":"Combining with Leader Election"},{"location":"1_Establishing_Consensus/5_SPEC/#stages-of-obtaining-consensus","text":"Pre-commit phase : Predliminary declaration of votes. - Can't vote on a ballot without preparing the transaction (n,x) and dropping (n', x') Individual Acceptance - Accept and broadcast to the other nodes that the individual acceptance has occured. Broadcast to all it's nodes about details ( in quorum slice ) Ratification : Nodes vote to accept a common update. - If more than quorum inside the quorum slice has been achieved, accept. If not correct, check if quorum achieved on any of the other data broadcast to all it's nodes about details of accepted transaction Finally, verify that transaction has been accepted in the other nodes. Broadcast : - Make the updated ledger and broadcast amongst all the nodes.","title":"Stages of obtaining Consensus"},{"location":"2_Secure Data/Data/","text":"Local Data Usage of a local database to store the data When it comes to most applications, they require a form of storage of data that can be queried easily, in a form that\u2019s accepted as a standard, for the best possible developer use. For this reason, we use SQLite databases, for the storage of data, at the node level. This allows us to present data in a form that is well understood, and an interface that developers are familiar with. Since we\u2019re not doing concurrent writes, we don\u2019t need a RDBMS database server. Ofcourse, we maintain a hash of the state of the database, as a snapshot capture. This may and may not be broadcast to the other nodes, according to security needs however, examining the hash of the state of the local database, against the ones of it\u2019s peer nodes allow us to understand node behavior. This method is especially helpful when there are other users with write access to the database, to avoid a state where the network\u2019s safety or fault tolerance may be affected.","title":"Local Data"},{"location":"2_Secure Data/Data/#local-data","text":"","title":"Local Data"},{"location":"2_Secure Data/Data/#usage-of-a-local-database-to-store-the-data","text":"When it comes to most applications, they require a form of storage of data that can be queried easily, in a form that\u2019s accepted as a standard, for the best possible developer use. For this reason, we use SQLite databases, for the storage of data, at the node level. This allows us to present data in a form that is well understood, and an interface that developers are familiar with. Since we\u2019re not doing concurrent writes, we don\u2019t need a RDBMS database server. Ofcourse, we maintain a hash of the state of the database, as a snapshot capture. This may and may not be broadcast to the other nodes, according to security needs however, examining the hash of the state of the local database, against the ones of it\u2019s peer nodes allow us to understand node behavior. This method is especially helpful when there are other users with write access to the database, to avoid a state where the network\u2019s safety or fault tolerance may be affected.","title":"Usage of a local database to store the data"},{"location":"2_Secure Data/Digital_Security/","text":"Verifying Authenticity Digital Signatures Digital Signatures using public key cryptography is extemenly essential to the TrstWeb ecosystem. Not only does it\u2019s usage result in the non-repudiation of the message , the digital signature also provides message authentication and data integrity. Message authentication \u2212 When the verifier validates the digital signature using public key of a sender, he is assured that signature has been created only by sender who possess the corresponding secret private key and no one else. Data Integrity \u2212 In case an attacker has access to the data and modifies it, the digital signature verification at receiver end fails. The hash of modified data and the output provided by the verification algorithm will not match. Hence, the receiver can safely deny the message assuming that data integrity has been breached. Non-repudiation \u2212 Since it is assumed that only the signer has the knowledge of the signature key, he can only create unique signature on a given data. Thus the receiver can present data and the digital signature to a third party as evidence if any dispute arises in the future. Therefore we can say that our cryptosystem now consists of the four essential elements of security namely \u2212 Privacy, Authentication, Integrity, and Non-repudiation.","title":"Verifying Authenticity"},{"location":"2_Secure Data/Digital_Security/#verifying-authenticity","text":"","title":"Verifying Authenticity"},{"location":"2_Secure Data/Digital_Security/#digital-signatures","text":"Digital Signatures using public key cryptography is extemenly essential to the TrstWeb ecosystem. Not only does it\u2019s usage result in the non-repudiation of the message , the digital signature also provides message authentication and data integrity. Message authentication \u2212 When the verifier validates the digital signature using public key of a sender, he is assured that signature has been created only by sender who possess the corresponding secret private key and no one else. Data Integrity \u2212 In case an attacker has access to the data and modifies it, the digital signature verification at receiver end fails. The hash of modified data and the output provided by the verification algorithm will not match. Hence, the receiver can safely deny the message assuming that data integrity has been breached. Non-repudiation \u2212 Since it is assumed that only the signer has the knowledge of the signature key, he can only create unique signature on a given data. Thus the receiver can present data and the digital signature to a third party as evidence if any dispute arises in the future. Therefore we can say that our cryptosystem now consists of the four essential elements of security namely \u2212 Privacy, Authentication, Integrity, and Non-repudiation.","title":"Digital Signatures"},{"location":"2_Secure Data/Merkle_Trees/","text":"Merkle Trees Merkle trees for verifiability. After nodes run for a while, the amount of data that\u2019s on the ledger is bound to grow A Merkle tree summarizes all the transactions in a block by producing a digital fingerprint of the entire set of transactions, thereby enabling a user to verify whether or not a transaction is included in a block. Merkle trees are created by repeatedly hashing pairs of nodes until there is only one hash left (this hash is called the Root Hash, or the Merkle Root). These Merkle trees are created from the geneis block, in a bottom up fashion, from hashes of individual states. Each leaf node is a hash of transactional data, and each non-leaf node is a hash of its previous hashes. Merkle trees are binary and therefore require an even number of leaf nodes. If the number of transactions is odd, the last hash will be duplicated once to create an even number of leaf nodes. Using Merkle trees substantially reduces the amount of data that each node has to maintain for verification purposes. At the end of the day : They help us verify the integrity of the data. Proofs are computationally cheap, therefore little disk and memory space neededt Reduces the amount of data that needs to be transmitted across the network.","title":"Merkle Trees"},{"location":"2_Secure Data/Merkle_Trees/#merkle-trees","text":"","title":"Merkle Trees"},{"location":"2_Secure Data/Merkle_Trees/#merkle-trees-for-verifiability","text":"After nodes run for a while, the amount of data that\u2019s on the ledger is bound to grow A Merkle tree summarizes all the transactions in a block by producing a digital fingerprint of the entire set of transactions, thereby enabling a user to verify whether or not a transaction is included in a block. Merkle trees are created by repeatedly hashing pairs of nodes until there is only one hash left (this hash is called the Root Hash, or the Merkle Root). These Merkle trees are created from the geneis block, in a bottom up fashion, from hashes of individual states. Each leaf node is a hash of transactional data, and each non-leaf node is a hash of its previous hashes. Merkle trees are binary and therefore require an even number of leaf nodes. If the number of transactions is odd, the last hash will be duplicated once to create an even number of leaf nodes. Using Merkle trees substantially reduces the amount of data that each node has to maintain for verification purposes. At the end of the day : They help us verify the integrity of the data. Proofs are computationally cheap, therefore little disk and memory space neededt Reduces the amount of data that needs to be transmitted across the network.","title":"Merkle trees for verifiability."},{"location":"2_Secure Data/OffChainData/","text":"Storing Files Off-chain data storage. Off-chain data is any non-transactional data that is too large to be stored in the blockchain efficiently. The best way to fully, uniformly, and securely control access to off-chain data is to create a shared network of storage and server resources that is designed to provide the required security and shared environment for the users of the network. Each time a data object is accessed it must be verified using stored hash values proving that it is the same object as the one that was stored initially. Every object stored in this filesystem, should be stored in more than one data store, such that the loss of a single data store doesn\u2019t result in the entire system to stop functioning as intended. e off-chain data is also be required.","title":"OffChainData"},{"location":"2_Secure Data/OffChainData/#storing-files","text":"","title":"Storing Files"},{"location":"2_Secure Data/OffChainData/#off-chain-data-storage","text":"Off-chain data is any non-transactional data that is too large to be stored in the blockchain efficiently. The best way to fully, uniformly, and securely control access to off-chain data is to create a shared network of storage and server resources that is designed to provide the required security and shared environment for the users of the network. Each time a data object is accessed it must be verified using stored hash values proving that it is the same object as the one that was stored initially. Every object stored in this filesystem, should be stored in more than one data store, such that the loss of a single data store doesn\u2019t result in the entire system to stop functioning as intended. e off-chain data is also be required.","title":"Off-chain data storage."},{"location":"3_Accessible_Compute_Infrastructure/5_ParallelCompute/","text":"Compute Middleware At proof of concept stage, we\u2019re working on creating a distributed virtual machine, that\u2019s based on the PVM model. A parallel virtual machine (PVM) is a distributed computing system that's created through a series of parallel computers, which are all merged together to be displayed as a unified virtual machine. This software framework creates a distributed computing architecture from a parallel connected system that works as a single unit to process any high-end computing task. Full spec in Feb '20.","title":"Compute Middleware"},{"location":"3_Accessible_Compute_Infrastructure/5_ParallelCompute/#compute-middleware","text":"At proof of concept stage, we\u2019re working on creating a distributed virtual machine, that\u2019s based on the PVM model. A parallel virtual machine (PVM) is a distributed computing system that's created through a series of parallel computers, which are all merged together to be displayed as a unified virtual machine. This software framework creates a distributed computing architecture from a parallel connected system that works as a single unit to process any high-end computing task. Full spec in Feb '20.","title":"Compute Middleware"},{"location":"4_TrstApps/3_Apps/","text":"Verifiable and Non-Censorable Applications The Domain Name System (DNS) is a hierarchical and decentralized naming system for computers, services, or other resources connected to the Internet or a private network. It associates various information with domain names assigned to each of the participating entities. Most prominently, it translates more readily memorized domain names to the numerical IP addresses needed for locating and identifying computer services and devices with the underlying network protocols. We\u2019re creating a network of decentralised DNS servers that are capable of replicating state as specified in the previous sections, such that it does not have any authorities. Every node is equal, and can have their own flexible trust model. Therefore, a system arises that\u2019s capable of not relying on centralised application layer sections of the internet, therefore, unless network packet communication is stopped from the transport layer, the applications can\u2019t be censored. For verifiability, open source software being served over the web are also encouraged to maintain scrutinizable code in the form of Off chain storage, with verifiable hashes every time they\u2019re served. This helps promote an ecosystem of trust. Full spec in Feb '20.","title":"Verifiable and Non-Censorable Applications"},{"location":"4_TrstApps/3_Apps/#verifiable-and-non-censorable-applications","text":"The Domain Name System (DNS) is a hierarchical and decentralized naming system for computers, services, or other resources connected to the Internet or a private network. It associates various information with domain names assigned to each of the participating entities. Most prominently, it translates more readily memorized domain names to the numerical IP addresses needed for locating and identifying computer services and devices with the underlying network protocols. We\u2019re creating a network of decentralised DNS servers that are capable of replicating state as specified in the previous sections, such that it does not have any authorities. Every node is equal, and can have their own flexible trust model. Therefore, a system arises that\u2019s capable of not relying on centralised application layer sections of the internet, therefore, unless network packet communication is stopped from the transport layer, the applications can\u2019t be censored. For verifiability, open source software being served over the web are also encouraged to maintain scrutinizable code in the form of Off chain storage, with verifiable hashes every time they\u2019re served. This helps promote an ecosystem of trust. Full spec in Feb '20.","title":"Verifiable and Non-Censorable Applications"},{"location":"5_Philosophy/0_Philosophy/","text":"Philosophy of our Design Decisions When it came to designing something new, we found it best to outline the general philosophy beforehand, such that each of our design or implementation details don't deviate far from what's necessary. These are the 9 cardinal rules for the design of the TrstWeb, thereby optimizing for the following properties: Ease of Use : Decentralised applications should be as intuitive and easy to use as current internet based applications, if not easier. We do this by introducing key design abstractions, making sure complexities are left to the system, and don\u2019t affect the user experience. Ease of Development : Decentralised applications should be as simple to develop as current internet applications. Moreover, deployment of such applications should be as simple as with traditional internet applications. Scalability : Decentralized applications should support users at internet-scale, i.e., hundreds of millions to billions of users. To do so, the underlying technology (including the blockchain) must scale with the number of users and applications it runs. End to End User Control : The applications running, should, by design provide control of the data and usage to the user. Simplicity : As an ecosystem, the basic underlying principles must be as simple as possible, such that any new user can get introduced and get up and running, in as little as a few minutes. Universality : a fundamental part of TRST's design philosophy is that we\u2019re creating the blockchain layer to be as simple and light as possible, yet such that it doesn\u2019t hamped functionality. Therefore, only fundamental building blocks shall be a part of the TRST layer, and the rest of the logic must be written out inside the applications. Modularity : The parts of the ecosystem should be designed to be as modular and separable as possible. This means that as a result, small modules can be updated and created with time. This ensures that feature updates and bug fixes keep getting rolled out, while still maintaining functionality. Non-discrimination and non-censorship : the ecosystem not attempt to actively restrict or prevent specific categories of usage, as long as the resource usages are being paid for and such a requirement exists amongst the users. Sufficient measures will be put in place that the technology percolates beyond censorship and restriction boundaries. We would like to thank the Solids Project, Bitcoin, Ethereum, and BlockStack for coming up with these policies, and imbibing them in their day to day activities.","title":"Philosophy of our Design Decisions"},{"location":"5_Philosophy/0_Philosophy/#philosophy-of-our-design-decisions","text":"When it came to designing something new, we found it best to outline the general philosophy beforehand, such that each of our design or implementation details don't deviate far from what's necessary. These are the 9 cardinal rules for the design of the TrstWeb, thereby optimizing for the following properties: Ease of Use : Decentralised applications should be as intuitive and easy to use as current internet based applications, if not easier. We do this by introducing key design abstractions, making sure complexities are left to the system, and don\u2019t affect the user experience. Ease of Development : Decentralised applications should be as simple to develop as current internet applications. Moreover, deployment of such applications should be as simple as with traditional internet applications. Scalability : Decentralized applications should support users at internet-scale, i.e., hundreds of millions to billions of users. To do so, the underlying technology (including the blockchain) must scale with the number of users and applications it runs. End to End User Control : The applications running, should, by design provide control of the data and usage to the user. Simplicity : As an ecosystem, the basic underlying principles must be as simple as possible, such that any new user can get introduced and get up and running, in as little as a few minutes. Universality : a fundamental part of TRST's design philosophy is that we\u2019re creating the blockchain layer to be as simple and light as possible, yet such that it doesn\u2019t hamped functionality. Therefore, only fundamental building blocks shall be a part of the TRST layer, and the rest of the logic must be written out inside the applications. Modularity : The parts of the ecosystem should be designed to be as modular and separable as possible. This means that as a result, small modules can be updated and created with time. This ensures that feature updates and bug fixes keep getting rolled out, while still maintaining functionality. Non-discrimination and non-censorship : the ecosystem not attempt to actively restrict or prevent specific categories of usage, as long as the resource usages are being paid for and such a requirement exists amongst the users. Sufficient measures will be put in place that the technology percolates beyond censorship and restriction boundaries. We would like to thank the Solids Project, Bitcoin, Ethereum, and BlockStack for coming up with these policies, and imbibing them in their day to day activities.","title":"Philosophy of our Design Decisions"},{"location":"Previous Works and Citations/9_citations/","text":"Citations Mazieres, D. (2015). The stellar consensus protocol: A federated model for internet-level consensus. Stellar Development Foundation, 32. Leslie Lamport. 1998. The Part-Time Parliament. 16, 2 (May 1998), 133\u2013169. Leslie Lamport. 2011a. Brief Announcement: Leaderless Byzantine Paxos. In Proceedings of the 25th International Conference on Distributed Computing. 141\u2013142. Leslie Lamport. 2011b. Byzantizing Paxos by Refinement. In Proceedings of the 25th International Conference on Distributed Computing. 211\u2013224. Leslie Lamport, Robert Shostak, and Marshall Pease. 1982. The Byzantine Generals Problem. ACM Transactions on Programing Languages and Systems 4, 3 (July 1982), 382\u2013401. Vitalik Buterin. 2014. Slasher: A Punitive Proof-of-Stake Algorithm. (January 2014). https://blog.ethereum.org/2014/01/15/slasher-a-punitive-proof-of-stake-algorithm/. Nakamoto, Satoshi. \"Bitcoin: A peer-to-peer electronic cash system.\" Consulted 1.2012 (2008): 28. Fischer, Michael J., Nancy A. Lynch, and Michael S. Paterson. \"Impossibility of distributed consensus with one faulty process.\" Journal of the ACM (JACM) 32.2 (1985): 374-382. Martin, J-P., and Lorenzo Alvisi. \"Fast byzantine consensus.\" Dependable and Secure Computing, IEEE Transactions on 3.3 (2006): 202-215. Alchieri, Eduardo AP, et al. \"Byzantine consensus with unknown participants.\" Principles of Distributed Systems. Springer Berlin Heidelberg, 2008. 22-40.","title":"Citations"},{"location":"Previous Works and Citations/9_citations/#citations","text":"Mazieres, D. (2015). The stellar consensus protocol: A federated model for internet-level consensus. Stellar Development Foundation, 32. Leslie Lamport. 1998. The Part-Time Parliament. 16, 2 (May 1998), 133\u2013169. Leslie Lamport. 2011a. Brief Announcement: Leaderless Byzantine Paxos. In Proceedings of the 25th International Conference on Distributed Computing. 141\u2013142. Leslie Lamport. 2011b. Byzantizing Paxos by Refinement. In Proceedings of the 25th International Conference on Distributed Computing. 211\u2013224. Leslie Lamport, Robert Shostak, and Marshall Pease. 1982. The Byzantine Generals Problem. ACM Transactions on Programing Languages and Systems 4, 3 (July 1982), 382\u2013401. Vitalik Buterin. 2014. Slasher: A Punitive Proof-of-Stake Algorithm. (January 2014). https://blog.ethereum.org/2014/01/15/slasher-a-punitive-proof-of-stake-algorithm/. Nakamoto, Satoshi. \"Bitcoin: A peer-to-peer electronic cash system.\" Consulted 1.2012 (2008): 28. Fischer, Michael J., Nancy A. Lynch, and Michael S. Paterson. \"Impossibility of distributed consensus with one faulty process.\" Journal of the ACM (JACM) 32.2 (1985): 374-382. Martin, J-P., and Lorenzo Alvisi. \"Fast byzantine consensus.\" Dependable and Secure Computing, IEEE Transactions on 3.3 (2006): 202-215. Alchieri, Eduardo AP, et al. \"Byzantine consensus with unknown participants.\" Principles of Distributed Systems. Springer Berlin Heidelberg, 2008. 22-40.","title":"Citations"}]}